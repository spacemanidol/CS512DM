\documentclass[11pt]{article}

\usepackage{alltt,fullpage,graphics,color,epsfig,amsmath, amssymb}
\usepackage{hyperref}
\usepackage{boxedminipage}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{commath}
\usepackage{graphicx}
\graphicspath{ {.} }
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\title{CS 512 Midterm}
\author{Daniel Campos}
\date{March 15th,2021}
\begin{document}
\maketitle
\section{Short Questions}
\subsection{If we train a classifier with few training samples then the classifier is less likely to overfit}
\textbf{False}. A smaller dataset is more easily skewed from the true data distribution than a larger one and as a result a classifier trained on a smaller sample of data is \textit{more} likely to overfit
\subsection{Self-training does not require labeled data}
\textbf{False}. Labeled data is required to train the initial classifier which then can perform self-training on the larger unlabeled data sample.
\subsection{Transfer learning is effective when the source and target task share many common grounds}
\textbf{True}. Take for example NLP or computer vision, effective transfer learning happens when the pretraining corpus resembles the fine tuning corpus. For computer vision is the structure of images, for NLP its the structure of language. 
\subsection{The results of two runs of K-means algorithm on the same dataset are expected to be the same regardless of different initialization.}
\textbf{False}. Initialization will completely skew what kind of results are found, popular methods like Forgy choose random points to be the centroids while Random Partition tends to chose points close to the data centroid. 
\subsection{In a transaction database, if the absolute support of the itemset X is larger than the absolute support of the itemset Y, then the relative support of the itemset X must be larger than the relative support of the itemset Y}
\textbf{True}. $support_{relative}(X) = (|X|/|T|)*100 ?= support_{relative}(Y) = (|Y|/|T|)*100$? Since $support_{abs}(X) = |X|,  support_{abs}(Y) = |Y|$ and $support_{abs}(X) > support_{abs}(Y)$ it implies $|X| > |Y|$ which since the transaction database size, $|T|$, is constant implies $support_{relative}(X) > support_{relative}(Y)$
\subsection{Explain why random walk with restart (RWR) is a good measure for node proximity in terms of catching information from both short and long distance?}
RWR is a good measure because it essentially measure the probability of reaching a given node from a start node. Since the algorithm has restarts we can discover long distance connections which can only be found by restarting and walking down a different path. 
\subsection{In frequent graph pattern mining, what is major computational cost for pattern growth approach? List one solution for this challenge.}
Since the algorithm finds patterns by building from the bottom up for long sequences this can have major overhead in memory. For example, if the target pattern is of length 100 then we must generate and store about $10^30$ candidates. Additionally, at each step of the algorithm we must scan the entire database which can be quite computationally expensive. To avoid the cost of scanning the entire database each time strategies like partitioning the database and using database projections. 
\subsection{What is the key difference between active learning and transductive learning?}
In transductive learning the amount of labeled data points does not change while in active learning the most salient points are sampled for human labeling. 
\subsection{After training for an SVM classifier is done, how is the testing/evaluation performed given a new data sample $x'$}
First, when we get the new data sample we should split it into equal portions of evaluation/validation to ensure we do not overfit based on our validation. Then, for each class in our classifier we rewrite each class specific MMH using the Lagrangian formulation to rewrite the decision boundary of our SVM as $d(X) = \sum_{i=1}^l y_i \alpha_i X'x_i+b$. We plug our test dataset and check to see the sign of the result, if its negative it is not in the class and if it is positive its in the class. We run this on the entire validation/evaluation set and we check what the precision and recall of correct class is across tasks.
\subsection{In Gaussian mixture model, how to alleviate the problem of local optima}
\end{document}